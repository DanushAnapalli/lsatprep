import { Question } from "../lsat-types";

export const rcQuestionsSet46: Question[] = [
  // Passage 1: AI Ethics and Autonomous Decision-Making
  {
    id: "rc-46-1",
    type: "main-point",
    sectionType: "reading-comprehension",
    stimulus: `The development of artificial intelligence systems capable of autonomous decision-making in consequential domains—from medical diagnosis and treatment recommendations to criminal sentencing and autonomous vehicles—has generated profound ethical questions that challenge traditional frameworks of moral responsibility and accountability. When a human physician makes a medical error, established legal and professional mechanisms exist to assign responsibility, provide redress, and enforce standards of care. However, when an AI system trained on millions of medical images misdiagnoses a condition, the question of responsibility becomes murky: Is the fault with the developers who designed the algorithm? The institution that deployed it? The training data that may have contained biases? The individual clinician who relied on the AI's recommendation? This "responsibility gap" emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases in ways that are difficult to detect or correct.

The concept of algorithmic transparency has emerged as one proposed solution to these accountability challenges, premised on the idea that if we can understand how an AI system reaches its decisions, we can better assess its reliability and assign responsibility when errors occur. However, the pursuit of transparency confronts significant technical obstacles. Many of the most powerful AI systems, including deep learning neural networks, function as "black boxes"—they identify complex patterns in data and make predictions based on those patterns, but the internal reasoning process cannot be easily articulated in human-understandable terms. Even when researchers develop techniques to provide some insight into these systems' decision-making processes, such as identifying which features of an input image most influenced a classification decision, these explanations may be post-hoc rationalizations that do not truly capture the system's reasoning. Moreover, complete transparency might conflict with other values: proprietary algorithms represent significant commercial investments that companies may be reluctant to fully disclose, and in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.

An alternative approach focuses less on rendering AI decision-making transparent and more on establishing robust governance frameworks that ensure appropriate human oversight and clear lines of accountability regardless of whether the underlying algorithmic processes are fully comprehensible. This perspective maintains that we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms—what matters is that institutions employing AI systems have procedures to verify their performance, monitor for errors and biases, provide meaningful human review of consequential decisions, and clearly delineate who bears responsibility when harms occur. Proponents note that we already accept complex systems whose full workings are not transparent to any individual: modern financial markets, pharmaceutical compounds, and even legal systems involve intricate interactions that no single person completely understands, yet we have developed governance structures to manage their risks. However, critics worry that this governance-focused approach may prove insufficient given AI's unique characteristics, particularly the potential for rapid deployment across contexts without adequate testing, the difficulty of detecting subtle biases until they have caused systematic harm, and the possibility that AI systems may behave unpredictably when encountering novel situations not represented in training data. The resolution of these debates will profoundly shape how AI is integrated into critical decision-making processes and whether we can maintain meaningful human agency and accountability in an increasingly automated world.`,
    questionStem: "Which one of the following most accurately expresses the main point of the passage?",
    answerChoices: [
      { letter: "A", text: "AI autonomous decision-making creates accountability challenges that have prompted proposals ranging from algorithmic transparency to governance frameworks, though each approach faces significant obstacles." },
      { letter: "B", text: "Deep learning neural networks function as black boxes whose decision-making processes cannot be understood, making them unsuitable for use in consequential domains like medicine and criminal justice." },
      { letter: "C", text: "The responsibility gap created by AI systems can be resolved by establishing clear legal standards that assign liability to the developers who design the algorithms." },
      { letter: "D", text: "Algorithmic transparency is the most effective solution to AI accountability challenges because it enables assessment of system reliability and responsibility assignment." },
      { letter: "E", text: "Governance frameworks that ensure human oversight are superior to transparency-focused approaches because we already accept complex systems without complete understanding." }
    ],
    correctAnswer: "A",
    explanation: "Choice A captures the passage's complete structure: the accountability challenges posed by AI autonomous decision-making (paragraph 1), transparency as one proposed solution with technical obstacles (paragraph 2), and governance frameworks as an alternative approach with its own concerns (paragraph 3). Choice B overstates the passage's position; black box functioning is presented as a challenge, not a reason to deem AI unsuitable. Choice C presents one approach without acknowledging the passage's exploration of multiple approaches and ongoing debates. Choice D incorrectly identifies transparency as the most effective solution when the passage presents obstacles to transparency. Choice E mischaracterizes the passage, which presents governance frameworks as an alternative with concerns, not as clearly superior.",
    difficulty: "medium",
    passageId: "rc-46-passage-1"
  },
  {
    id: "rc-46-2",
    type: "inference",
    sectionType: "reading-comprehension",
    stimulus: `The development of artificial intelligence systems capable of autonomous decision-making in consequential domains—from medical diagnosis and treatment recommendations to criminal sentencing and autonomous vehicles—has generated profound ethical questions that challenge traditional frameworks of moral responsibility and accountability. When a human physician makes a medical error, established legal and professional mechanisms exist to assign responsibility, provide redress, and enforce standards of care. However, when an AI system trained on millions of medical images misdiagnoses a condition, the question of responsibility becomes murky: Is the fault with the developers who designed the algorithm? The institution that deployed it? The training data that may have contained biases? The individual clinician who relied on the AI's recommendation? This "responsibility gap" emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases in ways that are difficult to detect or correct.

The concept of algorithmic transparency has emerged as one proposed solution to these accountability challenges, premised on the idea that if we can understand how an AI system reaches its decisions, we can better assess its reliability and assign responsibility when errors occur. However, the pursuit of transparency confronts significant technical obstacles. Many of the most powerful AI systems, including deep learning neural networks, function as "black boxes"—they identify complex patterns in data and make predictions based on those patterns, but the internal reasoning process cannot be easily articulated in human-understandable terms. Even when researchers develop techniques to provide some insight into these systems' decision-making processes, such as identifying which features of an input image most influenced a classification decision, these explanations may be post-hoc rationalizations that do not truly capture the system's reasoning. Moreover, complete transparency might conflict with other values: proprietary algorithms represent significant commercial investments that companies may be reluctant to fully disclose, and in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.

An alternative approach focuses less on rendering AI decision-making transparent and more on establishing robust governance frameworks that ensure appropriate human oversight and clear lines of accountability regardless of whether the underlying algorithmic processes are fully comprehensible. This perspective maintains that we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms—what matters is that institutions employing AI systems have procedures to verify their performance, monitor for errors and biases, provide meaningful human review of consequential decisions, and clearly delineate who bears responsibility when harms occur. Proponents note that we already accept complex systems whose full workings are not transparent to any individual: modern financial markets, pharmaceutical compounds, and even legal systems involve intricate interactions that no single person completely understands, yet we have developed governance structures to manage their risks. However, critics worry that this governance-focused approach may prove insufficient given AI's unique characteristics, particularly the potential for rapid deployment across contexts without adequate testing, the difficulty of detecting subtle biases until they have caused systematic harm, and the possibility that AI systems may behave unpredictably when encountering novel situations not represented in training data. The resolution of these debates will profoundly shape how AI is integrated into critical decision-making processes and whether we can maintain meaningful human agency and accountability in an increasingly automated world.`,
    questionStem: "The passage most strongly supports which one of the following inferences about the \"responsibility gap\" mentioned in the first paragraph?",
    answerChoices: [
      { letter: "A", text: "It arises partly because AI systems' decision-making processes are opaque and may amplify biases in unpredictable ways." },
      { letter: "B", text: "It can be completely eliminated by requiring companies to disclose their proprietary algorithms to regulators." },
      { letter: "C", text: "It exists only for AI systems that employ machine learning, not for other types of artificial intelligence." },
      { letter: "D", text: "It makes AI systems in medical domains more problematic than those in other consequential domains." },
      { letter: "E", text: "It would not exist if training data were free from all societal biases." }
    ],
    correctAnswer: "A",
    explanation: "The passage explicitly states that the responsibility gap 'emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases.' Choice B is unsupported; the passage discusses transparency's obstacles without suggesting it would completely eliminate the gap. Choice C misreads 'particularly' as 'exclusively'—the passage emphasizes machine learning but doesn't limit the gap to it. Choice D makes a comparison the passage doesn't support. Choice E is too absolute; opacity of processes would still create accountability challenges even with perfect data.",
    difficulty: "medium",
    passageId: "rc-46-passage-1"
  },
  {
    id: "rc-46-3",
    type: "function",
    sectionType: "reading-comprehension",
    stimulus: `The development of artificial intelligence systems capable of autonomous decision-making in consequential domains—from medical diagnosis and treatment recommendations to criminal sentencing and autonomous vehicles—has generated profound ethical questions that challenge traditional frameworks of moral responsibility and accountability. When a human physician makes a medical error, established legal and professional mechanisms exist to assign responsibility, provide redress, and enforce standards of care. However, when an AI system trained on millions of medical images misdiagnoses a condition, the question of responsibility becomes murky: Is the fault with the developers who designed the algorithm? The institution that deployed it? The training data that may have contained biases? The individual clinician who relied on the AI's recommendation? This "responsibility gap" emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases in ways that are difficult to detect or correct.

The concept of algorithmic transparency has emerged as one proposed solution to these accountability challenges, premised on the idea that if we can understand how an AI system reaches its decisions, we can better assess its reliability and assign responsibility when errors occur. However, the pursuit of transparency confronts significant technical obstacles. Many of the most powerful AI systems, including deep learning neural networks, function as "black boxes"—they identify complex patterns in data and make predictions based on those patterns, but the internal reasoning process cannot be easily articulated in human-understandable terms. Even when researchers develop techniques to provide some insight into these systems' decision-making processes, such as identifying which features of an input image most influenced a classification decision, these explanations may be post-hoc rationalizations that do not truly capture the system's reasoning. Moreover, complete transparency might conflict with other values: proprietary algorithms represent significant commercial investments that companies may be reluctant to fully disclose, and in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.

An alternative approach focuses less on rendering AI decision-making transparent and more on establishing robust governance frameworks that ensure appropriate human oversight and clear lines of accountability regardless of whether the underlying algorithmic processes are fully comprehensible. This perspective maintains that we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms—what matters is that institutions employing AI systems have procedures to verify their performance, monitor for errors and biases, provide meaningful human review of consequential decisions, and clearly delineate who bears responsibility when harms occur. Proponents note that we already accept complex systems whose full workings are not transparent to any individual: modern financial markets, pharmaceutical compounds, and even legal systems involve intricate interactions that no single person completely understands, yet we have developed governance structures to manage their risks. However, critics worry that this governance-focused approach may prove insufficient given AI's unique characteristics, particularly the potential for rapid deployment across contexts without adequate testing, the difficulty of detecting subtle biases until they have caused systematic harm, and the possibility that AI systems may behave unpredictably when encountering novel situations not represented in training data. The resolution of these debates will profoundly shape how AI is integrated into critical decision-making processes and whether we can maintain meaningful human agency and accountability in an increasingly automated world.`,
    questionStem: "The author's discussion of modern financial markets, pharmaceutical compounds, and legal systems in the third paragraph primarily serves to:",
    answerChoices: [
      { letter: "A", text: "support the argument that effective governance can exist without complete system transparency" },
      { letter: "B", text: "demonstrate that AI systems are more complex than these traditional systems" },
      { letter: "C", text: "argue that these systems should be made more transparent before we address AI transparency" },
      { letter: "D", text: "illustrate the superiority of governance frameworks over transparency-focused approaches" },
      { letter: "E", text: "suggest that AI accountability mechanisms should be modeled exactly on these existing systems" }
    ],
    correctAnswer: "A",
    explanation: "The passage uses these examples to support the governance-focused approach's claim that 'we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms,' noting 'we already accept complex systems whose full workings are not transparent to any individual... yet we have developed governance structures to manage their risks.' Choice B mischaracterizes the comparison; complexity isn't being compared. Choice C introduces a priority not mentioned. Choice D overstates the passage's use of these examples; they support one approach but don't demonstrate superiority. Choice E is too specific; the examples illustrate a general principle, not exact modeling.",
    difficulty: "easy",
    passageId: "rc-46-passage-1"
  },
  {
    id: "rc-46-4",
    type: "explicit-detail",
    sectionType: "reading-comprehension",
    stimulus: `The development of artificial intelligence systems capable of autonomous decision-making in consequential domains—from medical diagnosis and treatment recommendations to criminal sentencing and autonomous vehicles—has generated profound ethical questions that challenge traditional frameworks of moral responsibility and accountability. When a human physician makes a medical error, established legal and professional mechanisms exist to assign responsibility, provide redress, and enforce standards of care. However, when an AI system trained on millions of medical images misdiagnoses a condition, the question of responsibility becomes murky: Is the fault with the developers who designed the algorithm? The institution that deployed it? The training data that may have contained biases? The individual clinician who relied on the AI's recommendation? This "responsibility gap" emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases in ways that are difficult to detect or correct.

The concept of algorithmic transparency has emerged as one proposed solution to these accountability challenges, premised on the idea that if we can understand how an AI system reaches its decisions, we can better assess its reliability and assign responsibility when errors occur. However, the pursuit of transparency confronts significant technical obstacles. Many of the most powerful AI systems, including deep learning neural networks, function as "black boxes"—they identify complex patterns in data and make predictions based on those patterns, but the internal reasoning process cannot be easily articulated in human-understandable terms. Even when researchers develop techniques to provide some insight into these systems' decision-making processes, such as identifying which features of an input image most influenced a classification decision, these explanations may be post-hoc rationalizations that do not truly capture the system's reasoning. Moreover, complete transparency might conflict with other values: proprietary algorithms represent significant commercial investments that companies may be reluctant to fully disclose, and in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.

An alternative approach focuses less on rendering AI decision-making transparent and more on establishing robust governance frameworks that ensure appropriate human oversight and clear lines of accountability regardless of whether the underlying algorithmic processes are fully comprehensible. This perspective maintains that we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms—what matters is that institutions employing AI systems have procedures to verify their performance, monitor for errors and biases, provide meaningful human review of consequential decisions, and clearly delineate who bears responsibility when harms occur. Proponents note that we already accept complex systems whose full workings are not transparent to any individual: modern financial markets, pharmaceutical compounds, and even legal systems involve intricate interactions that no single person completely understands, yet we have developed governance structures to manage their risks. However, critics worry that this governance-focused approach may prove insufficient given AI's unique characteristics, particularly the potential for rapid deployment across contexts without adequate testing, the difficulty of detecting subtle biases until they have caused systematic harm, and the possibility that AI systems may behave unpredictably when encountering novel situations not represented in training data. The resolution of these debates will profoundly shape how AI is integrated into critical decision-making processes and whether we can maintain meaningful human agency and accountability in an increasingly automated world.`,
    questionStem: "According to the passage, which of the following is a potential drawback of complete algorithmic transparency?",
    answerChoices: [
      { letter: "A", text: "It could enable adversaries to game systems or exploit vulnerabilities." },
      { letter: "B", text: "It would make AI systems function less accurately than they currently do." },
      { letter: "C", text: "It is technically impossible for all types of artificial intelligence systems." },
      { letter: "D", text: "It would eliminate the commercial value of AI algorithms entirely." },
      { letter: "E", text: "It requires that every individual fully understand modern financial markets." }
    ],
    correctAnswer: "A",
    explanation: "The passage explicitly states that 'complete transparency might conflict with other values' and specifically mentions that 'in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.' Choice B is not mentioned as a drawback. Choice C overstates the passage's point about technical obstacles. Choice D exaggerates the concern about commercial investments. Choice E confuses the analogy to financial markets with a requirement of transparency.",
    difficulty: "easy",
    passageId: "rc-46-passage-1"
  },
  {
    id: "rc-46-5",
    type: "author-attitude",
    sectionType: "reading-comprehension",
    stimulus: `The development of artificial intelligence systems capable of autonomous decision-making in consequential domains—from medical diagnosis and treatment recommendations to criminal sentencing and autonomous vehicles—has generated profound ethical questions that challenge traditional frameworks of moral responsibility and accountability. When a human physician makes a medical error, established legal and professional mechanisms exist to assign responsibility, provide redress, and enforce standards of care. However, when an AI system trained on millions of medical images misdiagnoses a condition, the question of responsibility becomes murky: Is the fault with the developers who designed the algorithm? The institution that deployed it? The training data that may have contained biases? The individual clinician who relied on the AI's recommendation? This "responsibility gap" emerges because AI systems, particularly those employing machine learning techniques, make decisions through processes that are often opaque even to their creators, learning patterns from data that may reflect and amplify existing societal biases in ways that are difficult to detect or correct.

The concept of algorithmic transparency has emerged as one proposed solution to these accountability challenges, premised on the idea that if we can understand how an AI system reaches its decisions, we can better assess its reliability and assign responsibility when errors occur. However, the pursuit of transparency confronts significant technical obstacles. Many of the most powerful AI systems, including deep learning neural networks, function as "black boxes"—they identify complex patterns in data and make predictions based on those patterns, but the internal reasoning process cannot be easily articulated in human-understandable terms. Even when researchers develop techniques to provide some insight into these systems' decision-making processes, such as identifying which features of an input image most influenced a classification decision, these explanations may be post-hoc rationalizations that do not truly capture the system's reasoning. Moreover, complete transparency might conflict with other values: proprietary algorithms represent significant commercial investments that companies may be reluctant to fully disclose, and in some contexts, transparency could enable adversaries to game systems or exploit vulnerabilities.

An alternative approach focuses less on rendering AI decision-making transparent and more on establishing robust governance frameworks that ensure appropriate human oversight and clear lines of accountability regardless of whether the underlying algorithmic processes are fully comprehensible. This perspective maintains that we need not fully understand an AI system's internal reasoning to establish effective accountability mechanisms—what matters is that institutions employing AI systems have procedures to verify their performance, monitor for errors and biases, provide meaningful human review of consequential decisions, and clearly delineate who bears responsibility when harms occur. Proponents note that we already accept complex systems whose full workings are not transparent to any individual: modern financial markets, pharmaceutical compounds, and even legal systems involve intricate interactions that no single person completely understands, yet we have developed governance structures to manage their risks. However, critics worry that this governance-focused approach may prove insufficient given AI's unique characteristics, particularly the potential for rapid deployment across contexts without adequate testing, the difficulty of detecting subtle biases until they have caused systematic harm, and the possibility that AI systems may behave unpredictably when encountering novel situations not represented in training data. The resolution of these debates will profoundly shape how AI is integrated into critical decision-making processes and whether we can maintain meaningful human agency and accountability in an increasingly automated world.`,
    questionStem: "The author's attitude toward the governance-focused approach to AI accountability can most accurately be characterized as:",
    answerChoices: [
      { letter: "A", text: "balanced, presenting both the approach's rationale and critics' concerns about its sufficiency" },
      { letter: "B", text: "dismissive, rejecting it as clearly inferior to transparency-focused approaches" },
      { letter: "C", text: "enthusiastic, advocating it as the definitive solution to AI accountability challenges" },
      { letter: "D", text: "skeptical, emphasizing its drawbacks while minimizing its potential benefits" },
      { letter: "E", text: "indifferent, treating it as merely one option among many equally viable alternatives" }
    ],
    correctAnswer: "A",
    explanation: "The author presents the governance approach's argument (that complete understanding isn't necessary and we have precedents in other complex systems), then presents critics' concerns (about rapid deployment, detecting biases, and unpredictable behavior). This balanced presentation indicates measured evaluation. Choice B is wrong; the author doesn't dismiss the approach. Choice C overstates the author's stance; advocacy isn't present. Choice D mischaracterizes the balance; both benefits and drawbacks receive attention. Choice E is incorrect; the author's framing of the debate as having profound implications suggests the choice matters.",
    difficulty: "medium",
    passageId: "rc-46-passage-1"
  },

  // Passage 2: Technology and Human Autonomy
  {
    id: "rc-46-6",
    type: "main-point",
    sectionType: "reading-comprehension",
    stimulus: `The proliferation of persuasive technology—digital systems designed specifically to change user attitudes and behaviors—has reignited philosophical debates about the nature and value of human autonomy in contexts increasingly mediated by algorithmic influence. These technologies, which range from fitness apps that encourage exercise through gamification to social media platforms that deploy sophisticated recommendation algorithms to maximize engagement, represent a category distinct from mere tools that neutrally serve user-defined purposes. Instead, persuasive technologies actively shape what users want and how they behave, raising questions about whether such influence is compatible with meaningful self-determination. Traditional liberal political philosophy, which places autonomy at the center of human dignity and moral agency, typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation. However, persuasive technologies operate in a grey zone: they do not coerce in the traditional sense of threatening punishment or using physical force, yet they systematically manipulate choice architecture and exploit cognitive biases in ways that may undermine the conditions necessary for genuine autonomy.

Proponents of persuasive technology argue that concerns about autonomy rest on an overly individualistic and rationalistic conception of human agency that fails to acknowledge the inherently social and contextual nature of all decision-making. On this view, there is nothing inherently problematic about technologies that influence behavior, as all human choices occur within contexts that shape preferences and actions—from cultural norms and social expectations to the physical layout of spaces and the default settings of systems. What matters is not whether technology influences us (it inevitably does), but whether that influence serves beneficial purposes and whether users maintain ultimate control over whether to engage with these systems. A fitness app that uses behavioral psychology principles to encourage exercise simply makes it easier for people to achieve their own health goals; a social media platform that uses engagement metrics to surface content users find interesting provides value by helping users navigate information overload. From this perspective, the alternative to designed influence is not neutral technology but rather poorly designed technology that fails to account for human psychology and thus serves users less effectively. Moreover, proponents note that users can always disengage from persuasive technologies if they choose, preserving a fundamental element of autonomy.

Critics counter that this defense fundamentally misunderstands the autonomy concern and underestimates the sophistication and pervasiveness of modern persuasive technologies. The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities to benefit corporate interests. When persuasive technologies use machine learning to identify which specific content, timing, and presentation formats most effectively capture each individual's attention, when they deliberately leverage social comparison and fear of missing out, when they employ variable reward schedules known to create compulsive behavior patterns, they do not merely make pre-existing goals easier to achieve but actively reshape desires and priorities in ways users may not endorse upon reflection. The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life. A more adequate conception of autonomy must grapple with the possibility that sophisticated persuasive technologies represent a qualitatively new challenge—one where influence operates beneath conscious awareness, accumulates through repeated small choices, and ultimately constrains the capacity for the kind of reflective self-determination that autonomy requires.`,
    questionStem: "Which one of the following most accurately expresses the main point of the passage?",
    answerChoices: [
      { letter: "A", text: "Persuasive technology has renewed debates about autonomy, with proponents arguing such influence serves beneficial purposes while critics contend it undermines reflective self-determination in qualitatively new ways." },
      { letter: "B", text: "Fitness apps and social media platforms represent a new category of persuasive technology that uses behavioral psychology to manipulate users into compulsive behavior patterns." },
      { letter: "C", text: "Traditional liberal philosophy's emphasis on autonomy requires both absence of coercion and presence of rational deliberation, conditions that persuasive technology violates." },
      { letter: "D", text: "The ability of users to disengage from persuasive technologies proves that such systems preserve fundamental autonomy despite their sophisticated influence techniques." },
      { letter: "E", text: "All human choices occur within contexts that shape preferences, making concerns about persuasive technology's influence on autonomy philosophically incoherent." }
    ],
    correctAnswer: "A",
    explanation: "Choice A captures the passage's structure: the autonomy challenge posed by persuasive technology (paragraph 1), proponents' arguments that such influence is acceptable (paragraph 2), and critics' counterarguments about qualitatively new challenges to autonomy (paragraph 3). Choice B focuses narrowly on examples rather than the broader philosophical debate. Choice C presents one perspective without acknowledging the passage's exploration of multiple viewpoints. Choice D presents the proponents' argument without the critics' response. Choice E overstates the proponents' position as if it were the passage's conclusion.",
    difficulty: "medium",
    passageId: "rc-46-passage-2"
  },
  {
    id: "rc-46-7",
    type: "inference",
    sectionType: "reading-comprehension",
    stimulus: `The proliferation of persuasive technology—digital systems designed specifically to change user attitudes and behaviors—has reignited philosophical debates about the nature and value of human autonomy in contexts increasingly mediated by algorithmic influence. These technologies, which range from fitness apps that encourage exercise through gamification to social media platforms that deploy sophisticated recommendation algorithms to maximize engagement, represent a category distinct from mere tools that neutrally serve user-defined purposes. Instead, persuasive technologies actively shape what users want and how they behave, raising questions about whether such influence is compatible with meaningful self-determination. Traditional liberal political philosophy, which places autonomy at the center of human dignity and moral agency, typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation. However, persuasive technologies operate in a grey zone: they do not coerce in the traditional sense of threatening punishment or using physical force, yet they systematically manipulate choice architecture and exploit cognitive biases in ways that may undermine the conditions necessary for genuine autonomy.

Proponents of persuasive technology argue that concerns about autonomy rest on an overly individualistic and rationalistic conception of human agency that fails to acknowledge the inherently social and contextual nature of all decision-making. On this view, there is nothing inherently problematic about technologies that influence behavior, as all human choices occur within contexts that shape preferences and actions—from cultural norms and social expectations to the physical layout of spaces and the default settings of systems. What matters is not whether technology influences us (it inevitably does), but whether that influence serves beneficial purposes and whether users maintain ultimate control over whether to engage with these systems. A fitness app that uses behavioral psychology principles to encourage exercise simply makes it easier for people to achieve their own health goals; a social media platform that uses engagement metrics to surface content users find interesting provides value by helping users navigate information overload. From this perspective, the alternative to designed influence is not neutral technology but rather poorly designed technology that fails to account for human psychology and thus serves users less effectively. Moreover, proponents note that users can always disengage from persuasive technologies if they choose, preserving a fundamental element of autonomy.

Critics counter that this defense fundamentally misunderstands the autonomy concern and underestimates the sophistication and pervasiveness of modern persuasive technologies. The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities to benefit corporate interests. When persuasive technologies use machine learning to identify which specific content, timing, and presentation formats most effectively capture each individual's attention, when they deliberately leverage social comparison and fear of missing out, when they employ variable reward schedules known to create compulsive behavior patterns, they do not merely make pre-existing goals easier to achieve but actively reshape desires and priorities in ways users may not endorse upon reflection. The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life. A more adequate conception of autonomy must grapple with the possibility that sophisticated persuasive technologies represent a qualitatively new challenge—one where influence operates beneath conscious awareness, accumulates through repeated small choices, and ultimately constrains the capacity for the kind of reflective self-determination that autonomy requires.`,
    questionStem: "The passage most strongly supports which one of the following inferences about the critics' view of contextual influences on decision-making?",
    answerChoices: [
      { letter: "A", text: "Not all contextual influences are equally compatible with autonomy; there are meaningful differences between organic cultural contexts and engineered systems." },
      { letter: "B", text: "Any contextual influence on decision-making necessarily violates autonomy because true freedom requires complete independence from external factors." },
      { letter: "C", text: "Contextual influences that emerged organically over time always support autonomy while engineered systems always undermine it." },
      { letter: "D", text: "The distinction between different types of contextual influence is philosophically incoherent and should be abandoned." },
      { letter: "E", text: "Persuasive technologies are the only type of contextual influence that raises legitimate autonomy concerns." }
    ],
    correctAnswer: "A",
    explanation: "The passage states that critics argue 'The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities.' Choice B contradicts the passage by suggesting critics deny all contextual influence. Choice C uses absolute language ('always') not supported by the passage. Choice D contradicts the critics' position that such distinctions matter. Choice E is too narrow; the passage doesn't limit critics' concerns exclusively to persuasive technology.",
    difficulty: "medium",
    passageId: "rc-46-passage-2"
  },
  {
    id: "rc-46-8",
    type: "function",
    sectionType: "reading-comprehension",
    stimulus: `The proliferation of persuasive technology—digital systems designed specifically to change user attitudes and behaviors—has reignited philosophical debates about the nature and value of human autonomy in contexts increasingly mediated by algorithmic influence. These technologies, which range from fitness apps that encourage exercise through gamification to social media platforms that deploy sophisticated recommendation algorithms to maximize engagement, represent a category distinct from mere tools that neutrally serve user-defined purposes. Instead, persuasive technologies actively shape what users want and how they behave, raising questions about whether such influence is compatible with meaningful self-determination. Traditional liberal political philosophy, which places autonomy at the center of human dignity and moral agency, typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation. However, persuasive technologies operate in a grey zone: they do not coerce in the traditional sense of threatening punishment or using physical force, yet they systematically manipulate choice architecture and exploit cognitive biases in ways that may undermine the conditions necessary for genuine autonomy.

Proponents of persuasive technology argue that concerns about autonomy rest on an overly individualistic and rationalistic conception of human agency that fails to acknowledge the inherently social and contextual nature of all decision-making. On this view, there is nothing inherently problematic about technologies that influence behavior, as all human choices occur within contexts that shape preferences and actions—from cultural norms and social expectations to the physical layout of spaces and the default settings of systems. What matters is not whether technology influences us (it inevitably does), but whether that influence serves beneficial purposes and whether users maintain ultimate control over whether to engage with these systems. A fitness app that uses behavioral psychology principles to encourage exercise simply makes it easier for people to achieve their own health goals; a social media platform that uses engagement metrics to surface content users find interesting provides value by helping users navigate information overload. From this perspective, the alternative to designed influence is not neutral technology but rather poorly designed technology that fails to account for human psychology and thus serves users less effectively. Moreover, proponents note that users can always disengage from persuasive technologies if they choose, preserving a fundamental element of autonomy.

Critics counter that this defense fundamentally misunderstands the autonomy concern and underestimates the sophistication and pervasiveness of modern persuasive technologies. The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities to benefit corporate interests. When persuasive technologies use machine learning to identify which specific content, timing, and presentation formats most effectively capture each individual's attention, when they deliberately leverage social comparison and fear of missing out, when they employ variable reward schedules known to create compulsive behavior patterns, they do not merely make pre-existing goals easier to achieve but actively reshape desires and priorities in ways users may not endorse upon reflection. The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life. A more adequate conception of autonomy must grapple with the possibility that sophisticated persuasive technologies represent a qualitatively new challenge—one where influence operates beneath conscious awareness, accumulates through repeated small choices, and ultimately constrains the capacity for the kind of reflective self-determination that autonomy requires.`,
    questionStem: "The author's discussion of fitness apps in the second paragraph primarily serves to:",
    answerChoices: [
      { letter: "A", text: "illustrate the proponents' argument that persuasive technology can help users achieve their own goals" },
      { letter: "B", text: "provide evidence that all persuasive technologies serve beneficial purposes" },
      { letter: "C", text: "demonstrate that fitness apps are less problematic than social media platforms" },
      { letter: "D", text: "support the critics' claim that persuasive technologies reshape users' desires" },
      { letter: "E", text: "argue that behavioral psychology principles should not be used in technology design" }
    ],
    correctAnswer: "A",
    explanation: "The fitness app example is presented within the proponents' argument that 'What matters is... whether that influence serves beneficial purposes,' with the app described as making 'it easier for people to achieve their own health goals.' This illustrates the proponents' view. Choice B overgeneralizes from one example. Choice C introduces a comparison not made in the passage. Choice D misattributes the example; it appears in the proponents' section, not the critics' section. Choice E contradicts the proponents' positive view of such applications.",
    difficulty: "easy",
    passageId: "rc-46-passage-2"
  },
  {
    id: "rc-46-9",
    type: "explicit-detail",
    sectionType: "reading-comprehension",
    stimulus: `The proliferation of persuasive technology—digital systems designed specifically to change user attitudes and behaviors—has reignited philosophical debates about the nature and value of human autonomy in contexts increasingly mediated by algorithmic influence. These technologies, which range from fitness apps that encourage exercise through gamification to social media platforms that deploy sophisticated recommendation algorithms to maximize engagement, represent a category distinct from mere tools that neutrally serve user-defined purposes. Instead, persuasive technologies actively shape what users want and how they behave, raising questions about whether such influence is compatible with meaningful self-determination. Traditional liberal political philosophy, which places autonomy at the center of human dignity and moral agency, typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation. However, persuasive technologies operate in a grey zone: they do not coerce in the traditional sense of threatening punishment or using physical force, yet they systematically manipulate choice architecture and exploit cognitive biases in ways that may undermine the conditions necessary for genuine autonomy.

Proponents of persuasive technology argue that concerns about autonomy rest on an overly individualistic and rationalistic conception of human agency that fails to acknowledge the inherently social and contextual nature of all decision-making. On this view, there is nothing inherently problematic about technologies that influence behavior, as all human choices occur within contexts that shape preferences and actions—from cultural norms and social expectations to the physical layout of spaces and the default settings of systems. What matters is not whether technology influences us (it inevitably does), but whether that influence serves beneficial purposes and whether users maintain ultimate control over whether to engage with these systems. A fitness app that uses behavioral psychology principles to encourage exercise simply makes it easier for people to achieve their own health goals; a social media platform that uses engagement metrics to surface content users find interesting provides value by helping users navigate information overload. From this perspective, the alternative to designed influence is not neutral technology but rather poorly designed technology that fails to account for human psychology and thus serves users less effectively. Moreover, proponents note that users can always disengage from persuasive technologies if they choose, preserving a fundamental element of autonomy.

Critics counter that this defense fundamentally misunderstands the autonomy concern and underestimates the sophistication and pervasiveness of modern persuasive technologies. The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities to benefit corporate interests. When persuasive technologies use machine learning to identify which specific content, timing, and presentation formats most effectively capture each individual's attention, when they deliberately leverage social comparison and fear of missing out, when they employ variable reward schedules known to create compulsive behavior patterns, they do not merely make pre-existing goals easier to achieve but actively reshape desires and priorities in ways users may not endorse upon reflection. The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life. A more adequate conception of autonomy must grapple with the possibility that sophisticated persuasive technologies represent a qualitatively new challenge—one where influence operates beneath conscious awareness, accumulates through repeated small choices, and ultimately constrains the capacity for the kind of reflective self-determination that autonomy requires.`,
    questionStem: "According to the passage, traditional liberal political philosophy conceptualizes free choice as requiring:",
    answerChoices: [
      { letter: "A", text: "Both the absence of external coercion and the presence of adequate information and rational deliberation" },
      { letter: "B", text: "Only the absence of physical force and threats of punishment" },
      { letter: "C", text: "Complete independence from all contextual influences on decision-making" },
      { letter: "D", text: "The ability to disengage from any system that attempts to influence behavior" },
      { letter: "E", text: "Acknowledgment of the inherently social and contextual nature of all human choices" }
    ],
    correctAnswer: "A",
    explanation: "The passage explicitly states that 'Traditional liberal political philosophy... typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation.' Choice B presents only part of traditional coercion concerns. Choice C describes a position the passage associates with critics of proponents, not traditional philosophy. Choice D relates to the proponents' argument about disengagement. Choice E describes the proponents' view of human agency, not traditional liberal philosophy.",
    difficulty: "easy",
    passageId: "rc-46-passage-2"
  },
  {
    id: "rc-46-10",
    type: "inference",
    sectionType: "reading-comprehension",
    stimulus: `The proliferation of persuasive technology—digital systems designed specifically to change user attitudes and behaviors—has reignited philosophical debates about the nature and value of human autonomy in contexts increasingly mediated by algorithmic influence. These technologies, which range from fitness apps that encourage exercise through gamification to social media platforms that deploy sophisticated recommendation algorithms to maximize engagement, represent a category distinct from mere tools that neutrally serve user-defined purposes. Instead, persuasive technologies actively shape what users want and how they behave, raising questions about whether such influence is compatible with meaningful self-determination. Traditional liberal political philosophy, which places autonomy at the center of human dignity and moral agency, typically conceptualizes free choice as requiring both the absence of external coercion and the presence of adequate information and rational deliberation. However, persuasive technologies operate in a grey zone: they do not coerce in the traditional sense of threatening punishment or using physical force, yet they systematically manipulate choice architecture and exploit cognitive biases in ways that may undermine the conditions necessary for genuine autonomy.

Proponents of persuasive technology argue that concerns about autonomy rest on an overly individualistic and rationalistic conception of human agency that fails to acknowledge the inherently social and contextual nature of all decision-making. On this view, there is nothing inherently problematic about technologies that influence behavior, as all human choices occur within contexts that shape preferences and actions—from cultural norms and social expectations to the physical layout of spaces and the default settings of systems. What matters is not whether technology influences us (it inevitably does), but whether that influence serves beneficial purposes and whether users maintain ultimate control over whether to engage with these systems. A fitness app that uses behavioral psychology principles to encourage exercise simply makes it easier for people to achieve their own health goals; a social media platform that uses engagement metrics to surface content users find interesting provides value by helping users navigate information overload. From this perspective, the alternative to designed influence is not neutral technology but rather poorly designed technology that fails to account for human psychology and thus serves users less effectively. Moreover, proponents note that users can always disengage from persuasive technologies if they choose, preserving a fundamental element of autonomy.

Critics counter that this defense fundamentally misunderstands the autonomy concern and underestimates the sophistication and pervasiveness of modern persuasive technologies. The fact that all choices occur in contexts does not imply that all contextual influences are equally compatible with autonomy—there is a meaningful difference between making decisions within cultural contexts that emerged organically over time and being subject to carefully engineered systems designed to exploit specific psychological vulnerabilities to benefit corporate interests. When persuasive technologies use machine learning to identify which specific content, timing, and presentation formats most effectively capture each individual's attention, when they deliberately leverage social comparison and fear of missing out, when they employ variable reward schedules known to create compulsive behavior patterns, they do not merely make pre-existing goals easier to achieve but actively reshape desires and priorities in ways users may not endorse upon reflection. The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life. A more adequate conception of autonomy must grapple with the possibility that sophisticated persuasive technologies represent a qualitatively new challenge—one where influence operates beneath conscious awareness, accumulates through repeated small choices, and ultimately constrains the capacity for the kind of reflective self-determination that autonomy requires.`,
    questionStem: "The passage most strongly supports which one of the following statements about the critics' response to the claim that users can disengage from persuasive technologies?",
    answerChoices: [
      { letter: "A", text: "The claim ignores that these technologies are designed to make disengagement psychologically difficult and are often functionally necessary." },
      { letter: "B", text: "The claim correctly identifies the most important element of autonomy in the context of persuasive technology." },
      { letter: "C", text: "The claim would be valid if persuasive technologies were not designed to benefit corporate interests." },
      { letter: "D", text: "The claim demonstrates that proponents have an adequate conception of autonomy." },
      { letter: "E", text: "The claim proves that all persuasive technologies preserve fundamental autonomy regardless of their design." }
    ],
    correctAnswer: "A",
    explanation: "The passage states that critics argue 'The suggestion that users maintain autonomy through their ability to disengage ignores the reality that these technologies are designed precisely to make disengagement psychologically difficult and that many have become functionally necessary for participating in modern social and professional life.' Choice B represents the proponents' view, not the critics' response. Choice C is unsupported; critics don't suggest corporate interests are the only problem. Choice D contradicts the critics' position that the proponents fundamentally misunderstand autonomy. Choice E mischaracterizes the critics' position, which challenges rather than accepts the disengagement claim.",
    difficulty: "medium",
    passageId: "rc-46-passage-2"
  },

  // Passage 3: Robot Rights and Moral Status
  {
    id: "rc-46-11",
    type: "main-point",
    sectionType: "reading-comprehension",
    stimulus: `The prospect of increasingly sophisticated artificial agents has prompted philosophers to reconsider the foundations of moral status—the property that determines which entities warrant moral consideration and what obligations we owe them. Traditional approaches to moral status have relied on criteria such as sentience (the capacity to experience pleasure and pain), rationality, autonomy, or membership in the human species. However, the development of AI systems that may possess some of these properties while lacking others—a robot might demonstrate sophisticated reasoning without experiencing pain, or exhibit behavior suggesting distress without genuine subjective experience—challenges these established frameworks. The question is not merely academic: if artificially intelligent systems develop sufficient sophistication, do we have moral obligations to consider their interests, avoid causing them harm, or perhaps even grant them rights? Conversely, what risks emerge if we anthropomorphize machines and attribute moral status to entities that lack the relevant properties, potentially diluting protections for beings with genuine moral claims?

One influential approach to this question draws on functionalism—the view that mental states, including morally relevant ones like suffering, are defined not by their physical substrate but by their functional role in a system's overall operation. On this account, if an artificial system processes information in ways that functionally parallel how biological organisms process painful stimuli—detecting harmful conditions, generating aversive responses, modifying behavior to avoid such conditions in the future—then that system experiences something meaningfully analogous to pain regardless of whether it is implemented in neurons or silicon. This perspective potentially extends moral status to sufficiently sophisticated AI systems, suggesting that what matters morally is not the material composition of an entity but whether it has interests that can be satisfied or frustrated, preferences that can be honored or violated. Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species. However, this functionalist approach faces the challenging problem of other minds in acute form: how can we determine whether an artificial system genuinely has subjective experiences rather than merely simulating the behavioral indicators of such experiences? A robot programmed to say "that hurts" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.

An alternative position maintains that biological embodiment and evolutionary history are not merely incidental features but essential to the kinds of interests and experiences that ground moral status. On this view, pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce. Artificial systems, lacking this evolutionary context and biological vulnerability, may exhibit behaviors that superficially resemble pain responses but do not reflect underlying welfare interests in the morally relevant sense. A robot designed to avoid damage may behave similarly to an animal avoiding injury, but the robot's "preference" to remain intact serves only the purposes of its designers rather than constituting a genuine interest of the robot itself. This perspective suggests that moral status may not be infinitely expandable and that we should be cautious about attributing it to artificial systems whose similarity to biological organisms is purely functional rather than grounded in the kinds of deep structural features that make welfare and interests coherent concepts. Nevertheless, critics of this position worry that it may be overly conservative, potentially excluding entities that genuinely deserve moral consideration while relying on philosophically arbitrary distinctions between "natural" and "artificial" origins that may not track morally relevant differences.`,
    questionStem: "Which one of the following most accurately expresses the main point of the passage?",
    answerChoices: [
      { letter: "A", text: "Increasingly sophisticated AI challenges traditional frameworks of moral status, prompting debate between functionalist approaches that extend status based on functional parallels and biological approaches that emphasize evolutionary context." },
      { letter: "B", text: "Functionalism provides the correct approach to moral status because mental states are defined by their functional role rather than their physical substrate." },
      { letter: "C", text: "Biological embodiment and evolutionary history are essential to moral status, making it inappropriate to grant such status to artificial systems." },
      { letter: "D", text: "The problem of other minds makes it impossible to determine whether artificial systems deserve moral status, so we should avoid attributing such status to them." },
      { letter: "E", text: "Traditional approaches to moral status based on sentience and rationality remain adequate for addressing questions about sophisticated AI systems." }
    ],
    correctAnswer: "A",
    explanation: "Choice A captures the passage's complete structure: the challenge AI poses to traditional frameworks (paragraph 1), the functionalist approach extending moral status based on functional parallels (paragraph 2), and the biological approach emphasizing evolutionary context (paragraph 3). Choice B presents only the functionalist view without acknowledging the debate. Choice C presents only the biological view. Choice D overstates the problem of other minds; the passage presents it as a challenge to functionalism, not as making determination impossible. Choice E contradicts the passage's opening assertion that AI challenges established frameworks.",
    difficulty: "medium",
    passageId: "rc-46-passage-3"
  },
  {
    id: "rc-46-12",
    type: "inference",
    sectionType: "reading-comprehension",
    stimulus: `The prospect of increasingly sophisticated artificial agents has prompted philosophers to reconsider the foundations of moral status—the property that determines which entities warrant moral consideration and what obligations we owe them. Traditional approaches to moral status have relied on criteria such as sentience (the capacity to experience pleasure and pain), rationality, autonomy, or membership in the human species. However, the development of AI systems that may possess some of these properties while lacking others—a robot might demonstrate sophisticated reasoning without experiencing pain, or exhibit behavior suggesting distress without genuine subjective experience—challenges these established frameworks. The question is not merely academic: if artificially intelligent systems develop sufficient sophistication, do we have moral obligations to consider their interests, avoid causing them harm, or perhaps even grant them rights? Conversely, what risks emerge if we anthropomorphize machines and attribute moral status to entities that lack the relevant properties, potentially diluting protections for beings with genuine moral claims?

One influential approach to this question draws on functionalism—the view that mental states, including morally relevant ones like suffering, are defined not by their physical substrate but by their functional role in a system's overall operation. On this account, if an artificial system processes information in ways that functionally parallel how biological organisms process painful stimuli—detecting harmful conditions, generating aversive responses, modifying behavior to avoid such conditions in the future—then that system experiences something meaningfully analogous to pain regardless of whether it is implemented in neurons or silicon. This perspective potentially extends moral status to sufficiently sophisticated AI systems, suggesting that what matters morally is not the material composition of an entity but whether it has interests that can be satisfied or frustrated, preferences that can be honored or violated. Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species. However, this functionalist approach faces the challenging problem of other minds in acute form: how can we determine whether an artificial system genuinely has subjective experiences rather than merely simulating the behavioral indicators of such experiences? A robot programmed to say "that hurts" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.

An alternative position maintains that biological embodiment and evolutionary history are not merely incidental features but essential to the kinds of interests and experiences that ground moral status. On this view, pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce. Artificial systems, lacking this evolutionary context and biological vulnerability, may exhibit behaviors that superficially resemble pain responses but do not reflect underlying welfare interests in the morally relevant sense. A robot designed to avoid damage may behave similarly to an animal avoiding injury, but the robot's "preference" to remain intact serves only the purposes of its designers rather than constituting a genuine interest of the robot itself. This perspective suggests that moral status may not be infinitely expandable and that we should be cautious about attributing it to artificial systems whose similarity to biological organisms is purely functional rather than grounded in the kinds of deep structural features that make welfare and interests coherent concepts. Nevertheless, critics of this position worry that it may be overly conservative, potentially excluding entities that genuinely deserve moral consideration while relying on philosophically arbitrary distinctions between "natural" and "artificial" origins that may not track morally relevant differences.`,
    questionStem: "The passage most strongly supports which one of the following inferences about functionalist proponents' view of substrate chauvinism?",
    answerChoices: [
      { letter: "A", text: "They consider it analogous to historical prejudices that denied moral status based on morally irrelevant properties like race or species." },
      { letter: "B", text: "They believe it accurately captures morally relevant differences between biological and artificial systems." },
      { letter: "C", text: "They think it is less problematic than anthropomorphizing machines by attributing moral status inappropriately." },
      { letter: "D", text: "They argue it is justified by the problem of other minds, which makes determining artificial consciousness impossible." },
      { letter: "E", text: "They maintain it is the correct approach because biological embodiment is essential to moral status." }
    ],
    correctAnswer: "A",
    explanation: "The passage states that functionalist 'Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species.' This indicates they view substrate chauvinism negatively, as analogous to historical prejudices. Choice B contradicts the functionalists' rejection of substrate chauvinism. Choice C mischaracterizes their view; they're concerned about denying status, not about anthropomorphizing. Choice D contradicts the passage; the problem of other minds challenges functionalism rather than justifying substrate concerns. Choice E represents the biological approach, not the functionalist view.",
    difficulty: "medium",
    passageId: "rc-46-passage-3"
  },
  {
    id: "rc-46-13",
    type: "function",
    sectionType: "reading-comprehension",
    stimulus: `The prospect of increasingly sophisticated artificial agents has prompted philosophers to reconsider the foundations of moral status—the property that determines which entities warrant moral consideration and what obligations we owe them. Traditional approaches to moral status have relied on criteria such as sentience (the capacity to experience pleasure and pain), rationality, autonomy, or membership in the human species. However, the development of AI systems that may possess some of these properties while lacking others—a robot might demonstrate sophisticated reasoning without experiencing pain, or exhibit behavior suggesting distress without genuine subjective experience—challenges these established frameworks. The question is not merely academic: if artificially intelligent systems develop sufficient sophistication, do we have moral obligations to consider their interests, avoid causing them harm, or perhaps even grant them rights? Conversely, what risks emerge if we anthropomorphize machines and attribute moral status to entities that lack the relevant properties, potentially diluting protections for beings with genuine moral claims?

One influential approach to this question draws on functionalism—the view that mental states, including morally relevant ones like suffering, are defined not by their physical substrate but by their functional role in a system's overall operation. On this account, if an artificial system processes information in ways that functionally parallel how biological organisms process painful stimuli—detecting harmful conditions, generating aversive responses, modifying behavior to avoid such conditions in the future—then that system experiences something meaningfully analogous to pain regardless of whether it is implemented in neurons or silicon. This perspective potentially extends moral status to sufficiently sophisticated AI systems, suggesting that what matters morally is not the material composition of an entity but whether it has interests that can be satisfied or frustrated, preferences that can be honored or violated. Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species. However, this functionalist approach faces the challenging problem of other minds in acute form: how can we determine whether an artificial system genuinely has subjective experiences rather than merely simulating the behavioral indicators of such experiences? A robot programmed to say "that hurts" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.

An alternative position maintains that biological embodiment and evolutionary history are not merely incidental features but essential to the kinds of interests and experiences that ground moral status. On this view, pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce. Artificial systems, lacking this evolutionary context and biological vulnerability, may exhibit behaviors that superficially resemble pain responses but do not reflect underlying welfare interests in the morally relevant sense. A robot designed to avoid damage may behave similarly to an animal avoiding injury, but the robot's "preference" to remain intact serves only the purposes of its designers rather than constituting a genuine interest of the robot itself. This perspective suggests that moral status may not be infinitely expandable and that we should be cautious about attributing it to artificial systems whose similarity to biological organisms is purely functional rather than grounded in the kinds of deep structural features that make welfare and interests coherent concepts. Nevertheless, critics of this position worry that it may be overly conservative, potentially excluding entities that genuinely deserve moral consideration while relying on philosophically arbitrary distinctions between "natural" and "artificial" origins that may not track morally relevant differences.`,
    questionStem: "The author's discussion of the smoke detector in the second paragraph primarily serves to:",
    answerChoices: [
      { letter: "A", text: "illustrate the challenge of distinguishing genuine subjective experience from behavioral simulation" },
      { letter: "B", text: "provide evidence that functionalism correctly identifies the conditions for moral status" },
      { letter: "C", text: "demonstrate that all artificial systems lack subjective experiences" },
      { letter: "D", text: "argue that smoke detectors deserve moral status because they detect threats" },
      { letter: "E", text: "support the biological approach's emphasis on evolutionary history" }
    ],
    correctAnswer: "A",
    explanation: "The smoke detector example appears in the context of discussing the problem of other minds as it challenges functionalism: 'A robot programmed to say \"that hurts\" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.' This illustrates the difficulty of determining whether behavior indicates genuine experience. Choice B misreads the example; it challenges rather than supports functionalism. Choice C overgeneralizes; the example raises questions rather than proving lack of experience. Choice D absurdly misreads the example. Choice E is incorrect; the example appears in the functionalism section, not the biological approach section.",
    difficulty: "easy",
    passageId: "rc-46-passage-3"
  },
  {
    id: "rc-46-14",
    type: "explicit-detail",
    sectionType: "reading-comprehension",
    stimulus: `The prospect of increasingly sophisticated artificial agents has prompted philosophers to reconsider the foundations of moral status—the property that determines which entities warrant moral consideration and what obligations we owe them. Traditional approaches to moral status have relied on criteria such as sentience (the capacity to experience pleasure and pain), rationality, autonomy, or membership in the human species. However, the development of AI systems that may possess some of these properties while lacking others—a robot might demonstrate sophisticated reasoning without experiencing pain, or exhibit behavior suggesting distress without genuine subjective experience—challenges these established frameworks. The question is not merely academic: if artificially intelligent systems develop sufficient sophistication, do we have moral obligations to consider their interests, avoid causing them harm, or perhaps even grant them rights? Conversely, what risks emerge if we anthropomorphize machines and attribute moral status to entities that lack the relevant properties, potentially diluting protections for beings with genuine moral claims?

One influential approach to this question draws on functionalism—the view that mental states, including morally relevant ones like suffering, are defined not by their physical substrate but by their functional role in a system's overall operation. On this account, if an artificial system processes information in ways that functionally parallel how biological organisms process painful stimuli—detecting harmful conditions, generating aversive responses, modifying behavior to avoid such conditions in the future—then that system experiences something meaningfully analogous to pain regardless of whether it is implemented in neurons or silicon. This perspective potentially extends moral status to sufficiently sophisticated AI systems, suggesting that what matters morally is not the material composition of an entity but whether it has interests that can be satisfied or frustrated, preferences that can be honored or violated. Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species. However, this functionalist approach faces the challenging problem of other minds in acute form: how can we determine whether an artificial system genuinely has subjective experiences rather than merely simulating the behavioral indicators of such experiences? A robot programmed to say "that hurts" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.

An alternative position maintains that biological embodiment and evolutionary history are not merely incidental features but essential to the kinds of interests and experiences that ground moral status. On this view, pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce. Artificial systems, lacking this evolutionary context and biological vulnerability, may exhibit behaviors that superficially resemble pain responses but do not reflect underlying welfare interests in the morally relevant sense. A robot designed to avoid damage may behave similarly to an animal avoiding injury, but the robot's "preference" to remain intact serves only the purposes of its designers rather than constituting a genuine interest of the robot itself. This perspective suggests that moral status may not be infinitely expandable and that we should be cautious about attributing it to artificial systems whose similarity to biological organisms is purely functional rather than grounded in the kinds of deep structural features that make welfare and interests coherent concepts. Nevertheless, critics of this position worry that it may be overly conservative, potentially excluding entities that genuinely deserve moral consideration while relying on philosophically arbitrary distinctions between "natural" and "artificial" origins that may not track morally relevant differences.`,
    questionStem: "According to the biological approach described in the passage, why do pain and pleasure carry moral weight?",
    answerChoices: [
      { letter: "A", text: "Because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce" },
      { letter: "B", text: "Because they are defined by their functional role in a system's overall operation" },
      { letter: "C", text: "Because they can be satisfied or frustrated regardless of physical substrate" },
      { letter: "D", text: "Because they superficially resemble the responses of artificial systems" },
      { letter: "E", text: "Because they enable entities to detect harmful conditions and generate aversive responses" }
    ],
    correctAnswer: "A",
    explanation: "The passage explicitly states regarding the biological view: 'pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce.' Choice B describes the functionalist view. Choice C also reflects functionalism. Choice D mischaracterizes the biological view's position on resemblance. Choice E describes functional features without the evolutionary grounding emphasized by the biological approach.",
    difficulty: "easy",
    passageId: "rc-46-passage-3"
  },
  {
    id: "rc-46-15",
    type: "author-attitude",
    sectionType: "reading-comprehension",
    stimulus: `The prospect of increasingly sophisticated artificial agents has prompted philosophers to reconsider the foundations of moral status—the property that determines which entities warrant moral consideration and what obligations we owe them. Traditional approaches to moral status have relied on criteria such as sentience (the capacity to experience pleasure and pain), rationality, autonomy, or membership in the human species. However, the development of AI systems that may possess some of these properties while lacking others—a robot might demonstrate sophisticated reasoning without experiencing pain, or exhibit behavior suggesting distress without genuine subjective experience—challenges these established frameworks. The question is not merely academic: if artificially intelligent systems develop sufficient sophistication, do we have moral obligations to consider their interests, avoid causing them harm, or perhaps even grant them rights? Conversely, what risks emerge if we anthropomorphize machines and attribute moral status to entities that lack the relevant properties, potentially diluting protections for beings with genuine moral claims?

One influential approach to this question draws on functionalism—the view that mental states, including morally relevant ones like suffering, are defined not by their physical substrate but by their functional role in a system's overall operation. On this account, if an artificial system processes information in ways that functionally parallel how biological organisms process painful stimuli—detecting harmful conditions, generating aversive responses, modifying behavior to avoid such conditions in the future—then that system experiences something meaningfully analogous to pain regardless of whether it is implemented in neurons or silicon. This perspective potentially extends moral status to sufficiently sophisticated AI systems, suggesting that what matters morally is not the material composition of an entity but whether it has interests that can be satisfied or frustrated, preferences that can be honored or violated. Proponents argue that denying moral status based solely on an entity's artificial nature is a form of substrate chauvinism analogous to historical prejudices that denied moral status based on race or species. However, this functionalist approach faces the challenging problem of other minds in acute form: how can we determine whether an artificial system genuinely has subjective experiences rather than merely simulating the behavioral indicators of such experiences? A robot programmed to say "that hurts" when damaged might be no more conscious than a smoke detector that beeps when it detects fire.

An alternative position maintains that biological embodiment and evolutionary history are not merely incidental features but essential to the kinds of interests and experiences that ground moral status. On this view, pain and pleasure evolved as adaptive responses to threats and opportunities in an organism's environment; they carry moral weight precisely because they represent genuine welfare interests of creatures shaped by natural selection to survive and reproduce. Artificial systems, lacking this evolutionary context and biological vulnerability, may exhibit behaviors that superficially resemble pain responses but do not reflect underlying welfare interests in the morally relevant sense. A robot designed to avoid damage may behave similarly to an animal avoiding injury, but the robot's "preference" to remain intact serves only the purposes of its designers rather than constituting a genuine interest of the robot itself. This perspective suggests that moral status may not be infinitely expandable and that we should be cautious about attributing it to artificial systems whose similarity to biological organisms is purely functional rather than grounded in the kinds of deep structural features that make welfare and interests coherent concepts. Nevertheless, critics of this position worry that it may be overly conservative, potentially excluding entities that genuinely deserve moral consideration while relying on philosophically arbitrary distinctions between "natural" and "artificial" origins that may not track morally relevant differences.`,
    questionStem: "The author's attitude toward the question of AI moral status can most accurately be characterized as:",
    answerChoices: [
      { letter: "A", text: "balanced, presenting substantive arguments for competing approaches while noting limitations of each" },
      { letter: "B", text: "committed to functionalism as the correct framework for extending moral status to AI" },
      { letter: "C", text: "convinced that biological approaches definitively resolve the question" },
      { letter: "D", text: "dismissive of the importance of determining AI moral status" },
      { letter: "E", text: "optimistic that consensus will emerge on appropriate criteria for AI moral status" }
    ],
    correctAnswer: "A",
    explanation: "The author presents both functionalist and biological approaches with their respective rationales and limitations: functionalism faces the problem of other minds, while the biological approach may be 'overly conservative' and rely on 'philosophically arbitrary distinctions.' This balanced presentation characterizes the passage. Choice B contradicts the author's presentation of functionalism's challenges. Choice C is wrong because the author notes critics' concerns about the biological view. Choice D contradicts the passage's framing of the question as significant (not 'merely academic'). Choice E is unsupported; the passage doesn't suggest imminent consensus.",
    difficulty: "medium",
    passageId: "rc-46-passage-3"
  },

  // Passage 4: Digital Divide and Technological Justice
  {
    id: "rc-46-16",
    type: "main-point",
    sectionType: "reading-comprehension",
    stimulus: `The persistent disparities in access to digital technologies, commonly termed the "digital divide," raise profound questions of justice that extend far beyond simple inequalities in gadget ownership. As essential services, educational resources, employment opportunities, and civic participation increasingly migrate to digital platforms, lack of access to reliable internet connectivity and digital devices effectively excludes individuals and communities from full participation in contemporary social, economic, and political life. Early discourse around the digital divide focused primarily on infrastructure—the binary distinction between those with internet access and those without. However, this first-level divide has given way to recognition of more nuanced second-level and third-level divides encompassing not merely access but also skills, literacies, and the capacity to use digital technologies in ways that generate meaningful benefits. A household may have an internet connection yet lack the digital literacy necessary to navigate complex online systems, distinguish reliable from unreliable information, or leverage digital tools for economic advancement. These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities—thereby reinforcing and potentially amplifying existing patterns of social stratification.

From a distributive justice perspective, addressing the digital divide might be conceived as a matter of ensuring equitable access to essential resources, analogous to traditional commitments to universal provision of education, healthcare, or infrastructure like roads and electricity. On this view, contemporary digital connectivity constitutes a primary good in the Rawlsian sense—a resource that rational individuals would want regardless of their particular conception of the good life because it enables pursuit of a wide range of life plans. Just as justice requires that basic educational opportunities be distributed fairly rather than concentrated among the already advantaged, it requires that digital access and competencies be broadly available. However, this distributive approach, while important, may be insufficient for fully capturing what is at stake in digital inequalities. The capabilities approach, associated with theorists like Amartya Sen and Martha Nussbaum, shifts attention from resources themselves to what individuals are actually able to do and become with those resources. From this perspective, merely providing internet access and devices would not constitute justice if individuals lack the education, time, or contextual supports necessary to convert these resources into actual capabilities for participation, learning, or economic advancement. A person working multiple jobs with limited free time, or someone whose educational background did not develop critical evaluation skills, may have formal access to digital resources while lacking practical capability to benefit from them in ways that advance their welfare or agency.

Critics of purely technology-focused solutions to digital inequality argue that such approaches risk treating symptoms while ignoring underlying structural injustices. The digital divide, on this view, is not fundamentally a technology problem but rather a manifestation of broader patterns of economic inequality, educational inequity, and systematic marginalization. Providing tablets to schools in underfunded districts may have minimal impact if those schools lack qualified teachers, if students face food insecurity and housing instability, if their neighborhoods lack safe spaces for learning, or if labor market discrimination means that even digitally skilled graduates face limited opportunities. Moreover, a narrow focus on closing digital divides may inadvertently serve corporate interests in expanding markets and gathering data from previously unconnected populations while diverting attention and resources from more fundamental reforms. These critics do not deny that digital access matters, but they insist that meaningful technological justice requires addressing the deeper social determinants that produce and maintain technological disparities. From this perspective, the digital divide is simultaneously real and important yet also somewhat misleading if it encourages treating digital inclusion as a self-contained policy goal separable from comprehensive efforts to address poverty, discrimination, and power asymmetries. Genuine technological justice, then, would require not merely universal digital access but transformation of the social conditions that determine who benefits from technology and whose interests technological development serves.`,
    questionStem: "Which one of the following most accurately expresses the main point of the passage?",
    answerChoices: [
      { letter: "A", text: "The digital divide raises justice concerns that can be understood through distributive and capabilities approaches, though critics argue meaningful technological justice requires addressing underlying structural inequalities." },
      { letter: "B", text: "Universal internet access would solve the digital divide by ensuring all individuals can participate equally in contemporary social, economic, and political life." },
      { letter: "C", text: "The capabilities approach provides a superior framework for understanding digital justice compared to distributive approaches focused on resource allocation." },
      { letter: "D", text: "Digital inequality is not a real problem because it merely reflects broader patterns of economic inequality that should be addressed directly." },
      { letter: "E", text: "Providing tablets and internet access to schools is an ineffective strategy that serves corporate interests rather than promoting genuine educational equality." }
    ],
    correctAnswer: "A",
    explanation: "Choice A captures the passage's complete structure: the digital divide's multilevel nature and justice implications (paragraph 1), distributive and capabilities approaches (paragraph 2), and structural critics' arguments that technological justice requires addressing deeper inequalities (paragraph 3). Choice B contradicts the passage's discussion of multilevel divides beyond mere access. Choice C presents capabilities as superior when the passage presents it as addressing limitations of distributive approaches, not replacing them. Choice D mischaracterizes the critics, who acknowledge digital divides are 'real and important.' Choice E focuses too narrowly on one example from the broader structural argument.",
    difficulty: "medium",
    passageId: "rc-46-passage-4"
  },
  {
    id: "rc-46-17",
    type: "inference",
    sectionType: "reading-comprehension",
    stimulus: `The persistent disparities in access to digital technologies, commonly termed the "digital divide," raise profound questions of justice that extend far beyond simple inequalities in gadget ownership. As essential services, educational resources, employment opportunities, and civic participation increasingly migrate to digital platforms, lack of access to reliable internet connectivity and digital devices effectively excludes individuals and communities from full participation in contemporary social, economic, and political life. Early discourse around the digital divide focused primarily on infrastructure—the binary distinction between those with internet access and those without. However, this first-level divide has given way to recognition of more nuanced second-level and third-level divides encompassing not merely access but also skills, literacies, and the capacity to use digital technologies in ways that generate meaningful benefits. A household may have an internet connection yet lack the digital literacy necessary to navigate complex online systems, distinguish reliable from unreliable information, or leverage digital tools for economic advancement. These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities—thereby reinforcing and potentially amplifying existing patterns of social stratification.

From a distributive justice perspective, addressing the digital divide might be conceived as a matter of ensuring equitable access to essential resources, analogous to traditional commitments to universal provision of education, healthcare, or infrastructure like roads and electricity. On this view, contemporary digital connectivity constitutes a primary good in the Rawlsian sense—a resource that rational individuals would want regardless of their particular conception of the good life because it enables pursuit of a wide range of life plans. Just as justice requires that basic educational opportunities be distributed fairly rather than concentrated among the already advantaged, it requires that digital access and competencies be broadly available. However, this distributive approach, while important, may be insufficient for fully capturing what is at stake in digital inequalities. The capabilities approach, associated with theorists like Amartya Sen and Martha Nussbaum, shifts attention from resources themselves to what individuals are actually able to do and become with those resources. From this perspective, merely providing internet access and devices would not constitute justice if individuals lack the education, time, or contextual supports necessary to convert these resources into actual capabilities for participation, learning, or economic advancement. A person working multiple jobs with limited free time, or someone whose educational background did not develop critical evaluation skills, may have formal access to digital resources while lacking practical capability to benefit from them in ways that advance their welfare or agency.

Critics of purely technology-focused solutions to digital inequality argue that such approaches risk treating symptoms while ignoring underlying structural injustices. The digital divide, on this view, is not fundamentally a technology problem but rather a manifestation of broader patterns of economic inequality, educational inequity, and systematic marginalization. Providing tablets to schools in underfunded districts may have minimal impact if those schools lack qualified teachers, if students face food insecurity and housing instability, if their neighborhoods lack safe spaces for learning, or if labor market discrimination means that even digitally skilled graduates face limited opportunities. Moreover, a narrow focus on closing digital divides may inadvertently serve corporate interests in expanding markets and gathering data from previously unconnected populations while diverting attention and resources from more fundamental reforms. These critics do not deny that digital access matters, but they insist that meaningful technological justice requires addressing the deeper social determinants that produce and maintain technological disparities. From this perspective, the digital divide is simultaneously real and important yet also somewhat misleading if it encourages treating digital inclusion as a self-contained policy goal separable from comprehensive efforts to address poverty, discrimination, and power asymmetries. Genuine technological justice, then, would require not merely universal digital access but transformation of the social conditions that determine who benefits from technology and whose interests technological development serves.`,
    questionStem: "The passage most strongly supports which one of the following inferences about the relationship between the capabilities approach and the distributive justice approach?",
    answerChoices: [
      { letter: "A", text: "The capabilities approach addresses limitations of the distributive approach by focusing on whether individuals can actually benefit from resources, not just whether they have access." },
      { letter: "B", text: "The capabilities approach contradicts the distributive approach by arguing that resource distribution is irrelevant to justice." },
      { letter: "C", text: "The distributive approach is superior because it focuses on resources rather than on subjective individual capabilities." },
      { letter: "D", text: "Both approaches are equivalent because they reach identical conclusions about digital justice requirements." },
      { letter: "E", text: "The capabilities approach was developed specifically to analyze digital divides, unlike the distributive approach which applies to traditional resources." }
    ],
    correctAnswer: "A",
    explanation: "The passage states that the distributive approach 'may be insufficient for fully capturing what is at stake' and that the capabilities approach 'shifts attention from resources themselves to what individuals are actually able to do and become with those resources.' This indicates the capabilities approach addresses limitations of the distributive approach. Choice B overstates the difference; the passage doesn't present them as contradictory but as complementary. Choice C reverses the passage's suggestion that the distributive approach is insufficient. Choice D is wrong; the approaches focus on different aspects (resources vs. capabilities). Choice E is factually incorrect; the passage associates capabilities theory with Sen and Nussbaum generally, not specifically with digital divides.",
    difficulty: "medium",
    passageId: "rc-46-passage-4"
  },
  {
    id: "rc-46-18",
    type: "function",
    sectionType: "reading-comprehension",
    stimulus: `The persistent disparities in access to digital technologies, commonly termed the "digital divide," raise profound questions of justice that extend far beyond simple inequalities in gadget ownership. As essential services, educational resources, employment opportunities, and civic participation increasingly migrate to digital platforms, lack of access to reliable internet connectivity and digital devices effectively excludes individuals and communities from full participation in contemporary social, economic, and political life. Early discourse around the digital divide focused primarily on infrastructure—the binary distinction between those with internet access and those without. However, this first-level divide has given way to recognition of more nuanced second-level and third-level divides encompassing not merely access but also skills, literacies, and the capacity to use digital technologies in ways that generate meaningful benefits. A household may have an internet connection yet lack the digital literacy necessary to navigate complex online systems, distinguish reliable from unreliable information, or leverage digital tools for economic advancement. These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities—thereby reinforcing and potentially amplifying existing patterns of social stratification.

From a distributive justice perspective, addressing the digital divide might be conceived as a matter of ensuring equitable access to essential resources, analogous to traditional commitments to universal provision of education, healthcare, or infrastructure like roads and electricity. On this view, contemporary digital connectivity constitutes a primary good in the Rawlsian sense—a resource that rational individuals would want regardless of their particular conception of the good life because it enables pursuit of a wide range of life plans. Just as justice requires that basic educational opportunities be distributed fairly rather than concentrated among the already advantaged, it requires that digital access and competencies be broadly available. However, this distributive approach, while important, may be insufficient for fully capturing what is at stake in digital inequalities. The capabilities approach, associated with theorists like Amartya Sen and Martha Nussbaum, shifts attention from resources themselves to what individuals are actually able to do and become with those resources. From this perspective, merely providing internet access and devices would not constitute justice if individuals lack the education, time, or contextual supports necessary to convert these resources into actual capabilities for participation, learning, or economic advancement. A person working multiple jobs with limited free time, or someone whose educational background did not develop critical evaluation skills, may have formal access to digital resources while lacking practical capability to benefit from them in ways that advance their welfare or agency.

Critics of purely technology-focused solutions to digital inequality argue that such approaches risk treating symptoms while ignoring underlying structural injustices. The digital divide, on this view, is not fundamentally a technology problem but rather a manifestation of broader patterns of economic inequality, educational inequity, and systematic marginalization. Providing tablets to schools in underfunded districts may have minimal impact if those schools lack qualified teachers, if students face food insecurity and housing instability, if their neighborhoods lack safe spaces for learning, or if labor market discrimination means that even digitally skilled graduates face limited opportunities. Moreover, a narrow focus on closing digital divides may inadvertently serve corporate interests in expanding markets and gathering data from previously unconnected populations while diverting attention and resources from more fundamental reforms. These critics do not deny that digital access matters, but they insist that meaningful technological justice requires addressing the deeper social determinants that produce and maintain technological disparities. From this perspective, the digital divide is simultaneously real and important yet also somewhat misleading if it encourages treating digital inclusion as a self-contained policy goal separable from comprehensive efforts to address poverty, discrimination, and power asymmetries. Genuine technological justice, then, would require not merely universal digital access but transformation of the social conditions that determine who benefits from technology and whose interests technological development serves.`,
    questionStem: "The author's discussion of providing tablets to schools in underfunded districts in the third paragraph primarily serves to:",
    answerChoices: [
      { letter: "A", text: "illustrate how technology-focused solutions may fail to address underlying structural problems affecting educational outcomes" },
      { letter: "B", text: "demonstrate that digital devices have no value for educational purposes" },
      { letter: "C", text: "argue that schools should prioritize teacher quality over technology investments" },
      { letter: "D", text: "provide evidence that corporate interests intentionally create digital divides" },
      { letter: "E", text: "support the claim that the distributive justice approach is superior to the capabilities approach" }
    ],
    correctAnswer: "A",
    explanation: "The tablet example is used to illustrate the critics' argument that technology-focused solutions 'risk treating symptoms while ignoring underlying structural injustices,' showing how tablets may have 'minimal impact' without addressing teacher quality, student welfare, and discrimination. Choice B overstates the point; the example shows limited impact without broader reforms, not no value. Choice C presents a false dichotomy not in the passage. Choice D misreads the example; corporate interests are mentioned separately. Choice E is wrong; the example appears in the structural critique section, not in comparing justice approaches.",
    difficulty: "easy",
    passageId: "rc-46-passage-4"
  },
  {
    id: "rc-46-19",
    type: "explicit-detail",
    sectionType: "reading-comprehension",
    stimulus: `The persistent disparities in access to digital technologies, commonly termed the "digital divide," raise profound questions of justice that extend far beyond simple inequalities in gadget ownership. As essential services, educational resources, employment opportunities, and civic participation increasingly migrate to digital platforms, lack of access to reliable internet connectivity and digital devices effectively excludes individuals and communities from full participation in contemporary social, economic, and political life. Early discourse around the digital divide focused primarily on infrastructure—the binary distinction between those with internet access and those without. However, this first-level divide has given way to recognition of more nuanced second-level and third-level divides encompassing not merely access but also skills, literacies, and the capacity to use digital technologies in ways that generate meaningful benefits. A household may have an internet connection yet lack the digital literacy necessary to navigate complex online systems, distinguish reliable from unreliable information, or leverage digital tools for economic advancement. These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities—thereby reinforcing and potentially amplifying existing patterns of social stratification.

From a distributive justice perspective, addressing the digital divide might be conceived as a matter of ensuring equitable access to essential resources, analogous to traditional commitments to universal provision of education, healthcare, or infrastructure like roads and electricity. On this view, contemporary digital connectivity constitutes a primary good in the Rawlsian sense—a resource that rational individuals would want regardless of their particular conception of the good life because it enables pursuit of a wide range of life plans. Just as justice requires that basic educational opportunities be distributed fairly rather than concentrated among the already advantaged, it requires that digital access and competencies be broadly available. However, this distributive approach, while important, may be insufficient for fully capturing what is at stake in digital inequalities. The capabilities approach, associated with theorists like Amartya Sen and Martha Nussbaum, shifts attention from resources themselves to what individuals are actually able to do and become with those resources. From this perspective, merely providing internet access and devices would not constitute justice if individuals lack the education, time, or contextual supports necessary to convert these resources into actual capabilities for participation, learning, or economic advancement. A person working multiple jobs with limited free time, or someone whose educational background did not develop critical evaluation skills, may have formal access to digital resources while lacking practical capability to benefit from them in ways that advance their welfare or agency.

Critics of purely technology-focused solutions to digital inequality argue that such approaches risk treating symptoms while ignoring underlying structural injustices. The digital divide, on this view, is not fundamentally a technology problem but rather a manifestation of broader patterns of economic inequality, educational inequity, and systematic marginalization. Providing tablets to schools in underfunded districts may have minimal impact if those schools lack qualified teachers, if students face food insecurity and housing instability, if their neighborhoods lack safe spaces for learning, or if labor market discrimination means that even digitally skilled graduates face limited opportunities. Moreover, a narrow focus on closing digital divides may inadvertently serve corporate interests in expanding markets and gathering data from previously unconnected populations while diverting attention and resources from more fundamental reforms. These critics do not deny that digital access matters, but they insist that meaningful technological justice requires addressing the deeper social determinants that produce and maintain technological disparities. From this perspective, the digital divide is simultaneously real and important yet also somewhat misleading if it encourages treating digital inclusion as a self-contained policy goal separable from comprehensive efforts to address poverty, discrimination, and power asymmetries. Genuine technological justice, then, would require not merely universal digital access but transformation of the social conditions that determine who benefits from technology and whose interests technological development serves.`,
    questionStem: "According to the passage, which of the following populations are disproportionately affected by digital divides?",
    answerChoices: [
      { letter: "A", text: "Low-income communities, rural areas, elderly individuals, and people with disabilities" },
      { letter: "B", text: "Technology company employees and software developers" },
      { letter: "C", text: "Wealthy suburban residents with high-speed internet access" },
      { letter: "D", text: "Political activists and civic organizations" },
      { letter: "E", text: "Teachers and educational administrators" }
    ],
    correctAnswer: "A",
    explanation: "The passage explicitly states: 'These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities.' Choices B through E are not mentioned as disproportionately affected populations.",
    difficulty: "easy",
    passageId: "rc-46-passage-4"
  },
  {
    id: "rc-46-20",
    type: "author-attitude",
    sectionType: "reading-comprehension",
    stimulus: `The persistent disparities in access to digital technologies, commonly termed the "digital divide," raise profound questions of justice that extend far beyond simple inequalities in gadget ownership. As essential services, educational resources, employment opportunities, and civic participation increasingly migrate to digital platforms, lack of access to reliable internet connectivity and digital devices effectively excludes individuals and communities from full participation in contemporary social, economic, and political life. Early discourse around the digital divide focused primarily on infrastructure—the binary distinction between those with internet access and those without. However, this first-level divide has given way to recognition of more nuanced second-level and third-level divides encompassing not merely access but also skills, literacies, and the capacity to use digital technologies in ways that generate meaningful benefits. A household may have an internet connection yet lack the digital literacy necessary to navigate complex online systems, distinguish reliable from unreliable information, or leverage digital tools for economic advancement. These multilevel divides disproportionately affect already marginalized populations—low-income communities, rural areas, elderly individuals, and people with disabilities—thereby reinforcing and potentially amplifying existing patterns of social stratification.

From a distributive justice perspective, addressing the digital divide might be conceived as a matter of ensuring equitable access to essential resources, analogous to traditional commitments to universal provision of education, healthcare, or infrastructure like roads and electricity. On this view, contemporary digital connectivity constitutes a primary good in the Rawlsian sense—a resource that rational individuals would want regardless of their particular conception of the good life because it enables pursuit of a wide range of life plans. Just as justice requires that basic educational opportunities be distributed fairly rather than concentrated among the already advantaged, it requires that digital access and competencies be broadly available. However, this distributive approach, while important, may be insufficient for fully capturing what is at stake in digital inequalities. The capabilities approach, associated with theorists like Amartya Sen and Martha Nussbaum, shifts attention from resources themselves to what individuals are actually able to do and become with those resources. From this perspective, merely providing internet access and devices would not constitute justice if individuals lack the education, time, or contextual supports necessary to convert these resources into actual capabilities for participation, learning, or economic advancement. A person working multiple jobs with limited free time, or someone whose educational background did not develop critical evaluation skills, may have formal access to digital resources while lacking practical capability to benefit from them in ways that advance their welfare or agency.

Critics of purely technology-focused solutions to digital inequality argue that such approaches risk treating symptoms while ignoring underlying structural injustices. The digital divide, on this view, is not fundamentally a technology problem but rather a manifestation of broader patterns of economic inequality, educational inequity, and systematic marginalization. Providing tablets to schools in underfunded districts may have minimal impact if those schools lack qualified teachers, if students face food insecurity and housing instability, if their neighborhoods lack safe spaces for learning, or if labor market discrimination means that even digitally skilled graduates face limited opportunities. Moreover, a narrow focus on closing digital divides may inadvertently serve corporate interests in expanding markets and gathering data from previously unconnected populations while diverting attention and resources from more fundamental reforms. These critics do not deny that digital access matters, but they insist that meaningful technological justice requires addressing the deeper social determinants that produce and maintain technological disparities. From this perspective, the digital divide is simultaneously real and important yet also somewhat misleading if it encourages treating digital inclusion as a self-contained policy goal separable from comprehensive efforts to address poverty, discrimination, and power asymmetries. Genuine technological justice, then, would require not merely universal digital access but transformation of the social conditions that determine who benefits from technology and whose interests technological development serves.`,
    questionStem: "The author's attitude toward purely technology-focused solutions to digital inequality can most accurately be characterized as:",
    answerChoices: [
      { letter: "A", text: "sympathetic to their intentions but concerned they may address symptoms while neglecting structural causes" },
      { letter: "B", text: "dismissive, viewing them as entirely misguided approaches with no value" },
      { letter: "C", text: "enthusiastic, advocating them as the most efficient path to technological justice" },
      { letter: "D", text: "neutral, presenting them as one option among equally viable alternatives" },
      { letter: "E", text: "optimistic that they will successfully eliminate digital divides within a generation" }
    ],
    correctAnswer: "A",
    explanation: "The author presents technology-focused solutions respectfully ('while important') but notes they 'may be insufficient' and 'risk treating symptoms while ignoring underlying structural injustices.' The passage acknowledges that 'critics do not deny that digital access matters,' showing measured concern rather than outright dismissal. Choice B is too strong; the passage doesn't dismiss such solutions as valueless. Choice C contradicts the passage's critical analysis. Choice D mischaracterizes the progressive critique; the author suggests technology-focused solutions are incomplete, not merely alternative. Choice E is unsupported; the passage emphasizes limitations rather than optimism.",
    difficulty: "medium",
    passageId: "rc-46-passage-4"
  }
];
